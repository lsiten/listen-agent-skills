#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™®éæ€§é—®é¢˜åˆ†ææ¨¡å—
åˆ†æå·¥å•é—®é¢˜æ˜¯å¦å¯èƒ½æ˜¯æ™®éæ€§é—®é¢˜ï¼ˆæŸä¸ªå›½å®¶/åœ°åŒºã€æŸä¸ªç¯å¢ƒä¸€å®šä¼šå‡ºç°çš„ï¼‰
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from utils import (
    get_ticket_dir,
    load_json_file,
    save_json_file
)
from mcp_handler import (
    generate_mcp_instructions,
    load_mcp_results,
    build_filter_expression
)
from signoz_schema import build_field_spec, SEVERITY_ERROR_VALUES


def extract_prevalence_features(ticket_info: Dict[str, Any], log_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»å·¥å•ä¿¡æ¯å’Œæ—¥å¿—åˆ†æä¸­æå–ç”¨äºæ™®éæ€§åˆ†æçš„ç‰¹å¾
    
    ç‰¹å¾åŒ…æ‹¬ï¼š
    - å›½å®¶/åœ°åŒºï¼ˆgeo.country_name, geo.city_nameï¼‰
    - ç¯å¢ƒï¼ˆservice.environmentï¼‰
    - æœåŠ¡ç‰ˆæœ¬ï¼ˆservice.versionï¼‰
    - æµè§ˆå™¨ç‰ˆæœ¬ï¼ˆbrowser.name, browser.versionï¼‰
    - é”™è¯¯ç±»å‹
    - æ¥å£è·¯å¾„ï¼ˆrequest.pathnameï¼‰
    
    Args:
        ticket_info: å·¥å•ä¿¡æ¯
        log_analysis: æ—¥å¿—åˆ†æç»“æœ
    
    Returns:
        æå–çš„ç‰¹å¾ä¿¡æ¯
    """
    features = {
        'geo': {},
        'environment': None,
        'service_version': None,
        'browser_info': {},
        'error_type': None,
        'api_path': None,
        'service_name': None
    }
    
    # ä»å·¥å•ä¿¡æ¯ä¸­æå–
    region_info = ticket_info.get('region_info', {})
    if region_info.get('country'):
        features['geo']['country'] = region_info['country']
        features['geo']['country_name'] = region_info.get('country_name') or region_info['country']
    if region_info.get('city'):
        features['geo']['city'] = region_info['city']
        features['geo']['city_name'] = region_info.get('city_name') or region_info['city']
    
    # ä»æ—¥å¿—åˆ†æä¸­æå–ï¼ˆå¦‚æœå·¥å•ä¿¡æ¯ä¸­æ²¡æœ‰ï¼‰
    if not features['geo'].get('country') and log_analysis:
        # å°è¯•ä»å…³é”®é”™è¯¯ä¸­æå–åœ°ç†ä½ç½®ä¿¡æ¯
        key_errors = log_analysis.get('key_errors', [])
        for error in key_errors[:5]:
            # è¿™é‡Œå¯ä»¥å°è¯•ä»é”™è¯¯ä¿¡æ¯ä¸­æå–åœ°ç†ä½ç½®ï¼Œä½†é€šå¸¸éœ€è¦ä»MCPç»“æœä¸­æå–
            pass
    
    # æå–ç¯å¢ƒä¿¡æ¯
    app_info = ticket_info.get('app_info', {})
    if app_info.get('environment'):
        features['environment'] = app_info['environment']
    elif app_info.get('env'):
        features['environment'] = app_info['env']
    
    # æå–æœåŠ¡ç‰ˆæœ¬
    if app_info.get('app_version'):
        features['service_version'] = app_info['app_version']
    elif app_info.get('version'):
        features['service_version'] = app_info['version']
    
    # æå–æµè§ˆå™¨ä¿¡æ¯
    browser_info = ticket_info.get('browser_info', {})
    if browser_info.get('browser.name'):
        features['browser_info']['name'] = browser_info['browser.name']
    if browser_info.get('browser.version'):
        features['browser_info']['version'] = browser_info['browser.version']
    
    # æå–é”™è¯¯ç±»å‹
    if log_analysis and log_analysis.get('error_types'):
        # è·å–æœ€å¸¸è§çš„é”™è¯¯ç±»å‹
        error_types = log_analysis['error_types']
        if error_types:
            features['error_type'] = max(error_types.items(), key=lambda x: x[1])[0]
    
    # æå–æ¥å£è·¯å¾„
    api_info = ticket_info.get('api_info', {})
    if api_info.get('pathname'):
        features['api_path'] = api_info['pathname']
    elif api_info.get('api_path'):
        features['api_path'] = api_info['api_path']
    
    # æå–æœåŠ¡åç§°
    services = ticket_info.get('services', [])
    if services:
        features['service_name'] = services[0] if isinstance(services, list) else services
    
    return features


def build_prevalence_query(
    features: Dict[str, Any],
    time_range: Dict[str, Any],
    signoz_config: Dict[str, Any],
    expand_time_range: bool = True
) -> Optional[Dict[str, Any]]:
    """
    æ„å»ºæ™®éæ€§é—®é¢˜æŸ¥è¯¢ï¼ˆä¸é™å®šç”¨æˆ·IDã€è®¾å¤‡IDï¼‰
    
    Args:
        features: æå–çš„ç‰¹å¾ä¿¡æ¯
        time_range: æ—¶é—´èŒƒå›´
        signoz_config: SigNozé…ç½®
        expand_time_range: æ˜¯å¦æ‰©å±•æ—¶é—´èŒƒå›´ï¼ˆç”¨äºè·å–æ›´å¤šæ ·æœ¬ï¼‰
    
    Returns:
        Query Builder v5æ ¼å¼çš„æŸ¥è¯¢
    """
    start_ms = time_range.get('start')
    end_ms = time_range.get('end')
    
    if not start_ms or not end_ms:
        return None
    
    # å¦‚æœæ‰©å±•æ—¶é—´èŒƒå›´ï¼Œæ‰©å±•åˆ°å‰å24å°æ—¶ï¼ˆç”¨äºè·å–æ›´å¤šæ ·æœ¬ï¼‰
    if expand_time_range:
        from datetime import datetime
        center_ms = (start_ms + end_ms) // 2
        start_ms = center_ms - (24 * 60 * 60 * 1000)  # 24å°æ—¶å‰
        end_ms = center_ms + (24 * 60 * 60 * 1000)  # 24å°æ—¶å
        # ç¡®ä¿ä¸è¶…å‡ºå½“å‰æ—¶é—´
        now_ms = int(datetime.now().timestamp() * 1000)
        if end_ms > now_ms:
            end_ms = now_ms
            start_ms = end_ms - (48 * 60 * 60 * 1000)  # ç¡®ä¿è‡³å°‘48å°æ—¶èŒƒå›´
    
    # æ„å»ºè¿‡æ»¤æ¡ä»¶åˆ—è¡¨ï¼ˆç”¨äºåç»­è½¬æ¢ä¸ºexpressionï¼‰
    filter_items = []
    
    # åŸºç¡€è¿‡æ»¤ï¼šé”™è¯¯æ—¥å¿—
    filter_items.append({
        'key': {'name': 'severity_text'},
        'value': SEVERITY_ERROR_VALUES,
        'op': 'in'
    })
    
    # æ·»åŠ æœåŠ¡è¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if features.get('service_name'):
        filter_items.append({
            'key': {'name': 'service.name'},
            'value': [features['service_name']] if isinstance(features['service_name'], str) else features['service_name'],
            'op': 'in'
        })
    
    # æ·»åŠ ç¯å¢ƒè¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if features.get('environment'):
        filter_items.append({
            'key': {'name': 'service.environment'},
            'value': [features['environment']],
            'op': 'in'
        })
    
    # æ·»åŠ æœåŠ¡ç‰ˆæœ¬è¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if features.get('service_version'):
        filter_items.append({
            'key': {'name': 'service.version'},
            'value': [features['service_version']],
            'op': 'in'
        })
    
    # æ·»åŠ åœ°ç†ä½ç½®è¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if features.get('geo', {}).get('country_name'):
        filter_items.append({
            'key': {'name': 'geo.country_name'},
            'value': [features['geo']['country_name']],
            'op': 'in'
        })
    
    if features.get('geo', {}).get('city_name'):
        filter_items.append({
            'key': {'name': 'geo.city_name'},
            'value': [features['geo']['city_name']],
            'op': 'in'
        })
    
    # æ·»åŠ æµè§ˆå™¨ä¿¡æ¯è¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if features.get('browser_info', {}).get('name'):
        filter_items.append({
            'key': {'name': 'browser.name'},
            'value': [features['browser_info']['name']],
            'op': 'in'
        })
    
    if features.get('browser_info', {}).get('version'):
        filter_items.append({
            'key': {'name': 'browser.version'},
            'value': [features['browser_info']['version']],
            'op': 'in'
        })
    
    # æ·»åŠ æ¥å£è·¯å¾„è¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if features.get('api_path'):
        api_path = features['api_path']
        if not api_path.startswith('/'):
            api_path = '/' + api_path
        filter_items.append({
            'key': {'name': 'request.pathname'},
            'value': [api_path],
            'op': 'in'
        })
    
    # âš ï¸ é‡è¦ï¼šä¸æ·»åŠ ç”¨æˆ·IDå’Œè®¾å¤‡IDè¿‡æ»¤ï¼Œç”¨äºè·å–æ‰€æœ‰ç›¸å…³é”™è¯¯
    
    # å°†è¿‡æ»¤æ¡ä»¶è½¬æ¢ä¸ºSQL-likeè¡¨è¾¾å¼
    filter_expression = build_filter_expression(filter_items)
    
    # æ„å»ºæŸ¥è¯¢
    query = {
        'schemaVersion': 'v1',
        'start': start_ms,
        'end': end_ms,
        'requestType': 'raw',
        'compositeQuery': {
            'queries': [
                {
                    'type': 'builder_query',
                    'spec': {
                        'name': 'A',
                        'signal': 'logs',
                        'disabled': False,
                        'limit': 500,  # å¢åŠ é™åˆ¶ä»¥è·å–æ›´å¤šæ ·æœ¬
                        'offset': 0,
                        'order': [
                            {
                                'key': {
                                    'name': 'timestamp'
                                },
                                'direction': 'desc'
                            }
                        ],
                        'selectFields': [
                            build_field_spec('service.name', 'logs'),
                            build_field_spec('service.environment', 'logs'),
                            build_field_spec('service.version', 'logs'),
                            build_field_spec('body', 'logs'),
                            build_field_spec('request.pathname', 'logs'),
                            build_field_spec('message', 'logs'),
                            build_field_spec('severity_text', 'logs'),
                            build_field_spec('timestamp', 'logs'),
                            build_field_spec('geo.country_name', 'logs'),
                            build_field_spec('geo.city_name', 'logs'),
                            build_field_spec('browser.name', 'logs'),
                            build_field_spec('browser.version', 'logs'),
                            build_field_spec('user.id', 'logs'),
                            build_field_spec('user.client_id', 'logs'),
                            build_field_spec('source.address', 'logs')
                        ],
                        'filter': {
                            'expression': filter_expression
                        } if filter_expression else None,
                        'having': {
                            'expression': ''
                        }
                    }
                }
            ]
        },
        'formatOptions': {
            'formatTableResultForUI': True,
            'fillGaps': False
        },
        'variables': {}
    }
    
    return query


def analyze_prevalence_results(
    prevalence_results: Dict[str, Any],
    ticket_info: Dict[str, Any],
    features: Dict[str, Any]
) -> Dict[str, Any]:
    """
    åˆ†ææ™®éæ€§æŸ¥è¯¢ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯æ™®éæ€§é—®é¢˜
    
    Args:
        prevalence_results: æ™®éæ€§æŸ¥è¯¢ç»“æœ
        ticket_info: å·¥å•ä¿¡æ¯
        features: æå–çš„ç‰¹å¾ä¿¡æ¯
    
    Returns:
        æ™®éæ€§åˆ†æç»“æœ
    """
    analysis = {
        'is_prevalent': False,
        'prevalence_level': 'unknown',  # 'unknown', 'low', 'medium', 'high', 'critical'
        'affected_count': 0,
        'affected_users': set(),
        'affected_devices': set(),
        'affected_countries': set(),
        'affected_cities': set(),
        'time_distribution': {},
        'key_indicators': [],
        'recommendation': ''
    }
    
    if not prevalence_results or 'queries_executed' not in prevalence_results:
        return analysis
    
    # ç»Ÿè®¡å—å½±å“çš„æ•°é‡
    for query_result in prevalence_results.get('queries_executed', []):
        result_data = query_result.get('result', {})
        rows = result_data.get('rows', [])
        
        if rows and isinstance(rows, list):
            analysis['affected_count'] += len(rows)
            
            for row in rows:
                if not isinstance(row, dict):
                    continue
                
                # æå–ç”¨æˆ·ID
                user_id = (
                    row.get('user.id') or
                    row.get('attributes', {}).get('user', {}).get('id') or
                    row.get('attributes', {}).get('user.id')
                )
                if user_id:
                    analysis['affected_users'].add(str(user_id))
                
                # æå–è®¾å¤‡ID
                client_id = (
                    row.get('user.client_id') or
                    row.get('attributes', {}).get('user', {}).get('client_id') or
                    row.get('attributes', {}).get('user.client_id')
                )
                if client_id:
                    analysis['affected_devices'].add(str(client_id))
                
                # æå–å›½å®¶
                country = (
                    row.get('geo.country_name') or
                    row.get('attributes', {}).get('geo', {}).get('country_name') or
                    row.get('attributes', {}).get('geo.country_name')
                )
                if country:
                    analysis['affected_countries'].add(str(country))
                
                # æå–åŸå¸‚
                city = (
                    row.get('geo.city_name') or
                    row.get('attributes', {}).get('geo', {}).get('city_name') or
                    row.get('attributes', {}).get('geo.city_name')
                )
                if city:
                    analysis['affected_cities'].add(str(city))
                
                # æå–æ—¶é—´æˆ³ï¼ˆç”¨äºæ—¶é—´åˆ†å¸ƒåˆ†æï¼‰
                timestamp = row.get('timestamp')
                if timestamp:
                    try:
                        from datetime import datetime
                        dt = datetime.fromtimestamp(int(timestamp) / 1000)
                        hour_key = dt.strftime('%Y-%m-%d %H:00')
                        analysis['time_distribution'][hour_key] = analysis['time_distribution'].get(hour_key, 0) + 1
                    except Exception:
                        pass
    
    # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
    analysis['affected_users'] = list(analysis['affected_users'])
    analysis['affected_devices'] = list(analysis['affected_devices'])
    analysis['affected_countries'] = list(analysis['affected_countries'])
    analysis['affected_cities'] = list(analysis['affected_cities'])
    
    # åˆ¤æ–­æ™®éæ€§çº§åˆ«
    unique_users = len(analysis['affected_users'])
    unique_devices = len(analysis['affected_devices'])
    unique_countries = len(analysis['affected_countries'])
    unique_cities = len(analysis['affected_cities'])
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯æ™®éæ€§é—®é¢˜
    # æ ‡å‡†ï¼š
    # - å½±å“è¶…è¿‡10ä¸ªç”¨æˆ·æˆ–è®¾å¤‡
    # - å½±å“è¶…è¿‡2ä¸ªå›½å®¶æˆ–5ä¸ªåŸå¸‚
    # - é”™è¯¯æ•°é‡è¶…è¿‡50ä¸ª
    if analysis['affected_count'] >= 50 or unique_users >= 10 or unique_devices >= 10:
        analysis['is_prevalent'] = True
        if unique_countries >= 2 or unique_cities >= 5:
            analysis['prevalence_level'] = 'critical'
        elif unique_countries >= 1 or unique_cities >= 3:
            analysis['prevalence_level'] = 'high'
        else:
            analysis['prevalence_level'] = 'medium'
    elif analysis['affected_count'] >= 20 or unique_users >= 5 or unique_devices >= 5:
        analysis['is_prevalent'] = True
        analysis['prevalence_level'] = 'medium'
    elif analysis['affected_count'] >= 10 or unique_users >= 3 or unique_devices >= 3:
        analysis['is_prevalent'] = True
        analysis['prevalence_level'] = 'low'
    
    # ç”Ÿæˆå…³é”®æŒ‡æ ‡
    if analysis['is_prevalent']:
        analysis['key_indicators'].append(f"å½±å“ {analysis['affected_count']} ä¸ªé”™è¯¯æ—¥å¿—")
        if unique_users > 0:
            analysis['key_indicators'].append(f"å½±å“ {unique_users} ä¸ªä¸åŒç”¨æˆ·")
        if unique_devices > 0:
            analysis['key_indicators'].append(f"å½±å“ {unique_devices} ä¸ªä¸åŒè®¾å¤‡")
        if unique_countries > 0:
            analysis['key_indicators'].append(f"å½±å“ {unique_countries} ä¸ªå›½å®¶/åœ°åŒº: {', '.join(analysis['affected_countries'][:5])}")
        if unique_cities > 0:
            analysis['key_indicators'].append(f"å½±å“ {unique_cities} ä¸ªåŸå¸‚: {', '.join(analysis['affected_cities'][:5])}")
    
    # ç”Ÿæˆå»ºè®®
    if analysis['is_prevalent']:
        if analysis['prevalence_level'] == 'critical':
            analysis['recommendation'] = "âš ï¸ **ä¸¥é‡æ™®éæ€§é—®é¢˜**ï¼šæ­¤é—®é¢˜å½±å“å¤šä¸ªå›½å®¶/åœ°åŒºçš„å¤§é‡ç”¨æˆ·ï¼Œå»ºè®®ç«‹å³é‡‡å–ç´§æ€¥æªæ–½ï¼Œè€ƒè™‘å›æ»šæˆ–å‘å¸ƒçƒ­ä¿®å¤ã€‚"
        elif analysis['prevalence_level'] == 'high':
            analysis['recommendation'] = "âš ï¸ **é«˜æ™®éæ€§é—®é¢˜**ï¼šæ­¤é—®é¢˜å½±å“å¤šä¸ªç”¨æˆ·å’Œè®¾å¤‡ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†ï¼Œè€ƒè™‘å‘å¸ƒä¿®å¤ç‰ˆæœ¬ã€‚"
        elif analysis['prevalence_level'] == 'medium':
            analysis['recommendation'] = "âš ï¸ **ä¸­ç­‰æ™®éæ€§é—®é¢˜**ï¼šæ­¤é—®é¢˜å½±å“ä¸€å®šæ•°é‡çš„ç”¨æˆ·ï¼Œå»ºè®®å°½å¿«å¤„ç†ã€‚"
        else:
            analysis['recommendation'] = "âš ï¸ **è½»å¾®æ™®éæ€§é—®é¢˜**ï¼šæ­¤é—®é¢˜å½±å“å°‘é‡ç”¨æˆ·ï¼Œå»ºè®®å…³æ³¨å¹¶å¤„ç†ã€‚"
    else:
        analysis['recommendation'] = "âœ… æ­¤é—®é¢˜ä¼¼ä¹æ˜¯å­¤ç«‹äº‹ä»¶ï¼Œå½±å“èŒƒå›´æœ‰é™ã€‚"
    
    return analysis


def analyze_prevalence(
    ticket_info: Dict[str, Any],
    log_analysis: Dict[str, Any],
    ticket_context: Dict[str, Any],
    project_path: str,
    ticket_id: str
) -> Dict[str, Any]:
    """
    åˆ†æå·¥å•é—®é¢˜æ˜¯å¦æ˜¯æ™®éæ€§é—®é¢˜
    
    Args:
        ticket_info: å·¥å•ä¿¡æ¯
        log_analysis: æ—¥å¿—åˆ†æç»“æœ
        ticket_context: å·¥å•ä¸Šä¸‹æ–‡
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        ticket_id: å·¥å•ID
    
    Returns:
        æ™®éæ€§åˆ†æç»“æœ
    """
    print("\nğŸ” åˆ†ææ™®éæ€§é—®é¢˜...")
    
    # æå–ç‰¹å¾
    features = extract_prevalence_features(ticket_info, log_analysis)
    
    # å³ä½¿ç‰¹å¾ä¿¡æ¯ä¸è¶³ï¼Œä¹Ÿä¿å­˜å·²æå–çš„ç‰¹å¾ä¿¡æ¯
    ticket_dir = get_ticket_dir(project_path, ticket_id)
    features_file = ticket_dir / 'prevalence_features.json'
    
    features_data = {
        'ticket_id': ticket_id,
        'features': features,
        'extracted_at': datetime.now().isoformat(),
        'source': {
            'ticket_info': ticket_info,
            'log_analysis_summary': {
                'error_count': log_analysis.get('error_count', 0),
                'error_types': log_analysis.get('error_types', {}),
                'services': log_analysis.get('services', [])
            } if log_analysis else {}
        }
    }
    
    if not any([
        features.get('geo'),
        features.get('environment'),
        features.get('service_version'),
        features.get('browser_info'),
        features.get('api_path'),
        features.get('service_name')
    ]):
        print("  âš ï¸  æ— æ³•æå–è¶³å¤Ÿçš„ç‰¹å¾ä¿¡æ¯è¿›è¡Œæ™®éæ€§åˆ†æ")
        # ä»ç„¶ä¿å­˜ç‰¹å¾ä¿¡æ¯
        if save_json_file(features_file, features_data):
            print(f"  âœ… ç‰¹å¾ä¿¡æ¯å·²ä¿å­˜ï¼ˆå³ä½¿ä¿¡æ¯ä¸è¶³ï¼‰: {features_file}")
        # åŒæ—¶æ›´æ–°ticket_context.json
        ticket_context_file = ticket_dir / 'ticket_context.json'
        ticket_context_data = load_json_file(ticket_context_file)
        if ticket_context_data:
            if 'prevalence_features' not in ticket_context_data:
                ticket_context_data['prevalence_features'] = {}
            ticket_context_data['prevalence_features'] = features
            ticket_context_data['prevalence_features_extracted_at'] = datetime.now().isoformat()
            save_json_file(ticket_context_file, ticket_context_data)
        return {
            'is_prevalent': False,
            'prevalence_level': 'unknown',
            'reason': 'ç‰¹å¾ä¿¡æ¯ä¸è¶³',
            'features': features,
            'features_file': str(features_file)
        }
    
    print(f"  âœ… æå–åˆ°ç‰¹å¾ä¿¡æ¯:")
    if features.get('geo'):
        print(f"     - åœ°ç†ä½ç½®: {features['geo'].get('country_name', '')}, {features['geo'].get('city_name', '')}")
    if features.get('environment'):
        print(f"     - ç¯å¢ƒ: {features['environment']}")
    if features.get('service_version'):
        print(f"     - æœåŠ¡ç‰ˆæœ¬: {features['service_version']}")
    if features.get('browser_info'):
        print(f"     - æµè§ˆå™¨: {features['browser_info'].get('name', '')} {features['browser_info'].get('version', '')}")
    if features.get('api_path'):
        print(f"     - æ¥å£è·¯å¾„: {features['api_path']}")
    
    # ä¿å­˜ç‰¹å¾ä¿¡æ¯ï¼ˆç‰¹å¾ä¿¡æ¯è¶³å¤Ÿçš„æƒ…å†µï¼‰
    print("  ğŸ’¾ ä¿å­˜ç‰¹å¾ä¿¡æ¯...")
    if save_json_file(features_file, features_data):
        print(f"  âœ… ç‰¹å¾ä¿¡æ¯å·²ä¿å­˜: {features_file}")
    else:
        print("  âš ï¸  ç‰¹å¾ä¿¡æ¯ä¿å­˜å¤±è´¥")
    
    # åŒæ—¶æ›´æ–°ticket_context.jsonï¼Œå°†ç‰¹å¾ä¿¡æ¯ä¹Ÿä¿å­˜åˆ°é‚£é‡Œ
    ticket_context_file = ticket_dir / 'ticket_context.json'
    ticket_context_data = load_json_file(ticket_context_file)
    if ticket_context_data:
        if 'prevalence_features' not in ticket_context_data:
            ticket_context_data['prevalence_features'] = {}
        ticket_context_data['prevalence_features'] = features
        ticket_context_data['prevalence_features_extracted_at'] = datetime.now().isoformat()
        if save_json_file(ticket_context_file, ticket_context_data):
            print(f"  âœ… ç‰¹å¾ä¿¡æ¯å·²æ›´æ–°åˆ°å·¥å•ä¸Šä¸‹æ–‡")
    
    # æ„å»ºæ™®éæ€§æŸ¥è¯¢
    time_range = ticket_context.get('time_range', {})
    signoz_config_file = Path(project_path) / '.production-issue-analyzer' / 'signoz_config.json'
    signoz_config = load_json_file(signoz_config_file)
    
    prevalence_query = build_prevalence_query(features, time_range, signoz_config, expand_time_range=True)
    
    if not prevalence_query:
        print("  âš ï¸  æ— æ³•æ„å»ºæ™®éæ€§æŸ¥è¯¢")
        return {
            'is_prevalent': False,
            'prevalence_level': 'unknown',
            'reason': 'æ— æ³•æ„å»ºæŸ¥è¯¢'
        }
    
    # ç”ŸæˆMCPæŒ‡ä»¤ï¼ˆç”¨äºAIæ‰§è¡Œï¼‰
    print("  ğŸ“‹ ç”Ÿæˆæ™®éæ€§æŸ¥è¯¢æŒ‡ä»¤...")
    ticket_dir = get_ticket_dir(project_path, ticket_id)
    prevalence_instructions_file = ticket_dir / 'prevalence_instructions.json'
    
    instructions = {
        'ticket_id': ticket_id,
        'query_type': 'prevalence_analysis',
        'time_range': {
            'start': prevalence_query['start'],
            'end': prevalence_query['end'],
            'start_display': datetime.fromtimestamp(prevalence_query['start'] / 1000).strftime('%Y-%m-%d %H:%M:%S'),
            'end_display': datetime.fromtimestamp(prevalence_query['end'] / 1000).strftime('%Y-%m-%d %H:%M:%S')
        },
        'features': features,
        'queries': [
            {
                'priority': 1,
                'tool': 'signoz_execute_builder_query',
                'params': {
                    'query': prevalence_query
                },
                'description': 'æ™®éæ€§é—®é¢˜æŸ¥è¯¢ï¼ˆä¸é™å®šç”¨æˆ·IDå’Œè®¾å¤‡IDï¼Œç”¨äºåˆ†æå½±å“èŒƒå›´ï¼‰'
            }
        ],
        'notes': """è¯·æ‰§è¡Œæ­¤æŸ¥è¯¢ä»¥åˆ†æé—®é¢˜çš„æ™®éæ€§ã€‚
æ­¤æŸ¥è¯¢ä¸é™å®šç”¨æˆ·IDå’Œè®¾å¤‡IDï¼Œç”¨äºè·å–æ‰€æœ‰ç¬¦åˆç‰¹å¾æ¡ä»¶çš„é”™è¯¯æ—¥å¿—ã€‚
æŸ¥è¯¢ç»“æœå°†ç”¨äºåˆ¤æ–­é—®é¢˜æ˜¯å¦æ˜¯æ™®éæ€§é—®é¢˜ï¼ˆå½±å“å¤šä¸ªç”¨æˆ·ã€è®¾å¤‡ã€å›½å®¶/åœ°åŒºï¼‰ã€‚
æŸ¥è¯¢ç»“æœä¿å­˜åˆ° prevalence_results.json æ–‡ä»¶ä¸­ã€‚
"""
    }
    
    if save_json_file(prevalence_instructions_file, instructions):
        print(f"  âœ… æ™®éæ€§æŸ¥è¯¢æŒ‡ä»¤å·²ç”Ÿæˆ: {prevalence_instructions_file}")
        print("  â³ è¯·AIæ‰§è¡Œæ­¤æŸ¥è¯¢ï¼Œå°†ç»“æœä¿å­˜åˆ° prevalence_results.json")
        return {
            'instructions_file': str(prevalence_instructions_file),
            'features': features,
            'features_file': str(features_file),
            'status': 'pending_ai_execution'
        }
    else:
        print("  âš ï¸  æ™®éæ€§æŸ¥è¯¢æŒ‡ä»¤ç”Ÿæˆå¤±è´¥")
        return {
            'is_prevalent': False,
            'prevalence_level': 'unknown',
            'reason': 'æŒ‡ä»¤ç”Ÿæˆå¤±è´¥',
            'features': features,
            'features_file': str(features_file) if 'features_file' in locals() else None
        }


def load_and_analyze_prevalence_results(
    project_path: str,
    ticket_id: str,
    ticket_info: Dict[str, Any],
    features: Dict[str, Any]
) -> Dict[str, Any]:
    """
    åŠ è½½å¹¶åˆ†ææ™®éæ€§æŸ¥è¯¢ç»“æœ
    
    Args:
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        ticket_id: å·¥å•ID
        ticket_info: å·¥å•ä¿¡æ¯
        features: æå–çš„ç‰¹å¾ä¿¡æ¯
    
    Returns:
        æ™®éæ€§åˆ†æç»“æœ
    """
    ticket_dir = get_ticket_dir(project_path, ticket_id)
    prevalence_results_file = ticket_dir / 'prevalence_results.json'
    
    prevalence_results = load_json_file(prevalence_results_file)
    if not prevalence_results:
        return {
            'is_prevalent': False,
            'prevalence_level': 'unknown',
            'reason': 'æŸ¥è¯¢ç»“æœä¸å­˜åœ¨'
        }
    
    # åˆ†æç»“æœ
    analysis = analyze_prevalence_results(prevalence_results, ticket_info, features)
    
    return analysis
