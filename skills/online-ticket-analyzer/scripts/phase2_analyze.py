#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µ2ï¼šç»¼åˆåˆ†ææ¨¡å—
å¤„ç†MCPç»“æœï¼Œåˆ†æä»£ç é€»è¾‘ï¼Œæ£€ç´¢å†å²ç»éªŒï¼Œç”Ÿæˆè§£å†³æ–¹æ¡ˆ
"""

import sys
import re
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
from utils import (
    get_ticket_dir,
    load_json_file,
    save_json_file,
    save_markdown_file,
    format_datetime
)
from experience_manager import search_history_experience, save_experience
from prevalence_analyzer import (
    analyze_prevalence,
    load_and_analyze_prevalence_results
)


def check_mcp_results_empty(mcp_results: Dict[str, Any]) -> Tuple[bool, list]:
    """
    æ£€æŸ¥MCPæŸ¥è¯¢ç»“æœæ˜¯å¦ä¸ºç©º
    
    Args:
        mcp_results: MCPæŸ¥è¯¢ç»“æœ
    
    Returns:
        (æ˜¯å¦ä¸ºç©º, è­¦å‘Šä¿¡æ¯åˆ—è¡¨)
    """
    if not mcp_results:
        return True, ["MCPæŸ¥è¯¢ç»“æœä¸å­˜åœ¨"]
    
    is_empty = True
    warnings = []
    
    # æ£€æŸ¥queries_executedï¼ˆQuery Builder v5æ ¼å¼ï¼‰
    queries_executed = mcp_results.get('queries_executed', [])
    if queries_executed:
        for query_result in queries_executed:
            result_data = query_result.get('result', {})
            rows = result_data.get('rows', [])
            data = result_data.get('data')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            if rows and isinstance(rows, list) and len(rows) > 0:
                is_empty = False
            elif data and isinstance(data, list) and len(data) > 0:
                is_empty = False
            
            # æ£€æŸ¥è­¦å‘Šä¿¡æ¯
            if 'warnings' in result_data:
                warnings.extend(result_data['warnings'])
            if 'note' in query_result:
                warnings.append(query_result['note'])
    
    # æ£€æŸ¥queriesï¼ˆæ—§æ ¼å¼ï¼‰
    queries = mcp_results.get('queries', [])
    if queries:
        for query_result in queries:
            data = query_result.get('data')
            if data:
                if isinstance(data, list) and len(data) > 0:
                    is_empty = False
                elif isinstance(data, dict) and data.get('result'):
                    result_list = data['result']
                    if isinstance(result_list, list) and len(result_list) > 0:
                        is_empty = False
    
    return is_empty, warnings


def generate_preliminary_analysis(
    ticket_info: Dict[str, Any],
    ticket_context: Dict[str, Any],
    mcp_results: Dict[str, Any],
    warnings: list
) -> str:
    """
    ç”Ÿæˆåˆæ­¥åˆ¤æ–­ï¼ˆå½“æŸ¥è¯¢ç»“æœä¸ºç©ºæ—¶ï¼‰
    
    Args:
        ticket_info: å·¥å•ä¿¡æ¯
        ticket_context: å·¥å•ä¸Šä¸‹æ–‡
        mcp_results: MCPæŸ¥è¯¢ç»“æœ
        warnings: è­¦å‘Šä¿¡æ¯åˆ—è¡¨
    
    Returns:
        åˆæ­¥åˆ¤æ–­æ–‡æœ¬
    """
    analysis_parts = []
    
    analysis_parts.append("# âš ï¸ åˆæ­¥åˆ¤æ–­")
    analysis_parts.append("")
    analysis_parts.append("**æŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´è§£å†³æ–¹æ¡ˆã€‚**")
    analysis_parts.append("")
    
    # åˆ†æå¯èƒ½çš„åŸå› 
    analysis_parts.append("## å¯èƒ½çš„åŸå› åˆ†æ")
    analysis_parts.append("")
    
    time_range = ticket_context.get('time_range', {})
    time_info = ticket_info.get('time_info', {})
    
    # æ£€æŸ¥æ—¶é—´èŒƒå›´
    if time_range.get('start') and time_range.get('end'):
        from datetime import datetime
        start_dt = datetime.fromtimestamp(time_range['start'] / 1000)
        end_dt = datetime.fromtimestamp(time_range['end'] / 1000)
        duration_hours = (end_dt - start_dt).total_seconds() / 3600
        
        if duration_hours < 1:
            analysis_parts.append("- âš ï¸ **æ—¶é—´èŒƒå›´è¿‡çª„**ï¼šæŸ¥è¯¢æ—¶é—´èŒƒå›´å°äº1å°æ—¶ï¼Œå¯èƒ½é—æ¼äº†ç›¸å…³æ—¥å¿—")
        elif duration_hours > 24:
            analysis_parts.append("- âš ï¸ **æ—¶é—´èŒƒå›´è¿‡å®½**ï¼šæŸ¥è¯¢æ—¶é—´èŒƒå›´è¶…è¿‡24å°æ—¶ï¼Œå¯èƒ½éœ€è¦æ›´ç²¾ç¡®çš„æ—¶é—´")
        else:
            analysis_parts.append(f"- âœ… æ—¶é—´èŒƒå›´ï¼š{time_range.get('start_display', '')} - {time_range.get('end_display', '')}ï¼ˆ{duration_hours:.1f}å°æ—¶ï¼‰")
    
    # æ£€æŸ¥æœåŠ¡åç§°
    services = ticket_info.get('services', [])
    if not services:
        analysis_parts.append("- âš ï¸ **ç¼ºå°‘æœåŠ¡åç§°**ï¼šæœªæŒ‡å®šæœåŠ¡åç§°ï¼Œå¯èƒ½æŸ¥è¯¢äº†æ‰€æœ‰æœåŠ¡ä½†æœªæ‰¾åˆ°åŒ¹é…çš„æ—¥å¿—")
    else:
        analysis_parts.append(f"- âœ… æœåŠ¡åç§°ï¼š{', '.join(services)}")
    
    # æ£€æŸ¥ç”¨æˆ·ä¿¡æ¯
    user_info = ticket_info.get('user_info', {})
    if not user_info.get('user.id') and not user_info.get('user_id'):
        analysis_parts.append("- âš ï¸ **ç¼ºå°‘ç”¨æˆ·ID**ï¼šæœªæä¾›ç”¨æˆ·IDï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®å®šä½ç”¨æˆ·ç›¸å…³æ—¥å¿—")
    else:
        user_id = user_info.get('user.id') or user_info.get('user_id')
        analysis_parts.append(f"- âœ… ç”¨æˆ·IDï¼š{user_id}")
    
    # æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
    device_info = ticket_info.get('device_info', {})
    if not device_info.get('user.client_id') and not device_info.get('client_id') and not device_info.get('device_id'):
        analysis_parts.append("- âš ï¸ **ç¼ºå°‘è®¾å¤‡ID**ï¼šæœªæä¾›è®¾å¤‡IDï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®å®šä½è®¾å¤‡ç›¸å…³æ—¥å¿—")
    else:
        device_id = device_info.get('user.client_id') or device_info.get('client_id') or device_info.get('device_id')
        analysis_parts.append(f"- âœ… è®¾å¤‡IDï¼š{device_id}")
    
    # æ£€æŸ¥æ¥å£ä¿¡æ¯
    api_info = ticket_info.get('api_info', {})
    if not api_info.get('pathname') and not api_info.get('api_path'):
        analysis_parts.append("- âš ï¸ **ç¼ºå°‘æ¥å£è·¯å¾„**ï¼šæœªæä¾›æ¥å£è·¯å¾„ï¼Œå¯èƒ½æ— æ³•ç²¾ç¡®å®šä½æ¥å£ç›¸å…³æ—¥å¿—")
    else:
        api_path = api_info.get('pathname') or api_info.get('api_path')
        analysis_parts.append(f"- âœ… æ¥å£è·¯å¾„ï¼š{api_path}")
    
    # æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
    if warnings:
        analysis_parts.append("")
        analysis_parts.append("## æŸ¥è¯¢è­¦å‘Šä¿¡æ¯")
        analysis_parts.append("")
        for warning in warnings:
            analysis_parts.append(f"- âš ï¸ {warning}")
    
    # æä¾›å»ºè®®
    analysis_parts.append("")
    analysis_parts.append("## ğŸ’¡ å»ºè®®")
    analysis_parts.append("")
    analysis_parts.append("ä¸ºäº†è·å¾—æ›´å‡†ç¡®çš„æŸ¥è¯¢ç»“æœï¼Œå»ºè®®æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š")
    analysis_parts.append("")
    
    suggestions = []
    if not services:
        suggestions.append("1. **æœåŠ¡åç§°**ï¼šæ˜ç¡®æŒ‡å®šå‘ç”Ÿé—®é¢˜çš„æœåŠ¡åç§°")
    if not user_info.get('user.id') and not user_info.get('user_id'):
        suggestions.append("2. **ç”¨æˆ·ID**ï¼šæä¾›ç”¨æˆ·IDï¼Œå¯ä»¥ç²¾ç¡®å®šä½ç”¨æˆ·ç›¸å…³æ—¥å¿—")
    if not device_info.get('user.client_id') and not device_info.get('client_id') and not device_info.get('device_id'):
        suggestions.append("3. **è®¾å¤‡ID**ï¼šæä¾›è®¾å¤‡IDæˆ–å®¢æˆ·ç«¯IDï¼Œå¯ä»¥ç²¾ç¡®å®šä½è®¾å¤‡ç›¸å…³æ—¥å¿—")
    if not api_info.get('pathname') and not api_info.get('api_path'):
        suggestions.append("4. **æ¥å£è·¯å¾„**ï¼šæä¾›å…·ä½“çš„æ¥å£è·¯å¾„ï¼Œå¯ä»¥ç²¾ç¡®å®šä½æ¥å£ç›¸å…³æ—¥å¿—")
    if time_range.get('start') and time_range.get('end'):
        from datetime import datetime
        start_dt = datetime.fromtimestamp(time_range['start'] / 1000)
        end_dt = datetime.fromtimestamp(time_range['end'] / 1000)
        duration_hours = (end_dt - start_dt).total_seconds() / 3600
        if duration_hours < 1:
            suggestions.append("5. **æ—¶é—´èŒƒå›´**ï¼šæ‰©å¤§æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼ˆå»ºè®®è‡³å°‘2å°æ—¶ï¼‰")
        elif duration_hours > 24:
            suggestions.append("5. **æ—¶é—´èŒƒå›´**ï¼šç¼©å°æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼Œæä¾›æ›´ç²¾ç¡®çš„é—®é¢˜å‘ç”Ÿæ—¶é—´")
    
    if not suggestions:
        suggestions.append("1. æ£€æŸ¥æ—¶é—´èŒƒå›´æ˜¯å¦æ­£ç¡®")
        suggestions.append("2. ç¡®è®¤æœåŠ¡åç§°æ˜¯å¦å‡†ç¡®ï¼ˆä½¿ç”¨list_servicesè·å–å®é™…æœåŠ¡åï¼‰")
        suggestions.append("3. å°è¯•ç®€åŒ–æŸ¥è¯¢æ¡ä»¶ï¼Œé€æ­¥æ·»åŠ è¿‡æ»¤æ¡ä»¶")
        if warnings:
            suggestions.append("4. æ£€æŸ¥æŸ¥è¯¢è­¦å‘Šä¿¡æ¯ï¼Œå¯èƒ½éœ€è¦æ˜ç¡®æŒ‡å®šå­—æ®µçš„fieldContextå’ŒfieldDataType")
    
    for suggestion in suggestions:
        analysis_parts.append(suggestion)
    
    analysis_parts.append("")
    analysis_parts.append("## ä¸‹ä¸€æ­¥æ“ä½œ")
    analysis_parts.append("")
    analysis_parts.append("è¯·æ ¹æ®ä¸Šè¿°å»ºè®®ï¼Œæä¾›æ›´ç²¾ç¡®çš„ä¿¡æ¯åï¼Œé‡æ–°è¿è¡Œåˆ†æã€‚")
    analysis_parts.append("")
    analysis_parts.append("æˆ–è€…ï¼Œå¦‚æœæ‚¨ç¡®è®¤ä¿¡æ¯æ— è¯¯ï¼Œå¯ä»¥ï¼š")
    analysis_parts.append("1. æ£€æŸ¥SigNozå¹³å°ï¼Œç¡®è®¤è¯¥æ—¶é—´æ®µå†…æ˜¯å¦æœ‰ç›¸å…³æ—¥å¿—")
    analysis_parts.append("2. ç¡®è®¤æœåŠ¡åç§°ã€æ—¶é—´èŒƒå›´ç­‰ä¿¡æ¯æ˜¯å¦æ­£ç¡®")
    analysis_parts.append("3. å¦‚æœç¡®å®æ²¡æœ‰æ—¥å¿—ï¼Œå¯èƒ½éœ€è¦æ‰©å¤§æ—¶é—´èŒƒå›´æˆ–æ£€æŸ¥å…¶ä»–æœåŠ¡")
    
    return "\n".join(analysis_parts)


def process_log_data(mcp_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¤„ç†æ—¥å¿—æ•°æ®ï¼Œæ·±åº¦åˆ†æï¼Œæå–å…³é”®é”™è¯¯ä¿¡æ¯
    
    æ³¨æ„ï¼šä¸å‘é€åŸå§‹æ—¥å¿—ç»™å¤§æ¨¡å‹ï¼Œåªæå–å…³é”®ä¿¡æ¯
    
    Args:
        mcp_results: MCPæŸ¥è¯¢ç»“æœ
    
    Returns:
        å¤„ç†åçš„æ—¥å¿—åˆ†æç»“æœ
    """
    log_analysis = {
        'error_count': 0,
        'error_types': {},
        'services': set(),
        'key_errors': [],
        'time_pattern': {},
        'summary': '',
        'has_data': False
    }
    
    # å¤„ç†æŸ¥è¯¢ç»“æœ
    if 'queries_executed' in mcp_results:
        for query_result in mcp_results.get('queries_executed', []):
            result_data = query_result.get('result', {})
            rows = result_data.get('rows', [])
            if rows and isinstance(rows, list) and len(rows) > 0:
                log_analysis['has_data'] = True
                for row in rows:
                    process_log_entry(row, log_analysis)
    
    if 'queries' in mcp_results:
        for query_result in mcp_results.get('queries', []):
            process_query_result(query_result, log_analysis)
            if query_result.get('data'):
                data = query_result['data']
                if isinstance(data, list) and len(data) > 0:
                    log_analysis['has_data'] = True
                elif isinstance(data, dict) and data.get('result'):
                    result_list = data['result']
                    if isinstance(result_list, list) and len(result_list) > 0:
                        log_analysis['has_data'] = True
    
    # è½¬æ¢setä¸ºlist
    log_analysis['services'] = list(log_analysis['services'])
    
    # ç”Ÿæˆæ‘˜è¦
    log_analysis['summary'] = generate_log_summary(log_analysis)
    
    return log_analysis


def process_query_result(query_result: Dict[str, Any], log_analysis: Dict[str, Any]) -> None:
    """å¤„ç†å•ä¸ªæŸ¥è¯¢ç»“æœ"""
    # å¤„ç†queries_executedæ ¼å¼ï¼ˆQuery Builder v5ï¼‰
    if 'result' in query_result:
        result_data = query_result.get('result', {})
        rows = result_data.get('rows', [])
        if rows and isinstance(rows, list):
            for row in rows:
                process_log_entry(row, log_analysis)
            return
    
    # å¤„ç†æ—§æ ¼å¼ï¼ˆqueriesï¼‰
    if 'data' not in query_result:
        return
    
    data = query_result['data']
    
    # å¤„ç†åˆ—è¡¨ç±»å‹çš„ç»“æœ
    if isinstance(data, list):
        for entry in data:
            process_log_entry(entry, log_analysis)
    elif isinstance(data, dict):
        # å¤„ç†å­—å…¸ç±»å‹çš„ç»“æœ
        if 'result' in data:
            result_data = data['result']
            if isinstance(result_data, list):
                for entry in result_data:
                    process_log_entry(entry, log_analysis)


def process_log_entry(entry: Dict[str, Any], log_analysis: Dict[str, Any]) -> None:
    """å¤„ç†å•ä¸ªæ—¥å¿—æ¡ç›®"""
    # æå–æœåŠ¡åï¼ˆä¼˜å…ˆä»resourcesä¸­æå–ï¼Œæ³¨æ„å®é™…å­—æ®µæ˜¯resources.service.nameï¼‰
    service_name = (
        extract_field_value(entry, 'resources.service.name') or
        extract_field_value(entry, 'resource.service.name') or
        extract_field_value(entry, 'service.name') or
        extract_field_value(entry, 'service_name')
    )
    if service_name:
        log_analysis['services'].add(service_name)
    
    # æå–é”™è¯¯ä¿¡æ¯ï¼ˆä»attributesä¸­æå–ï¼‰
    severity = (
        extract_field_value(entry, 'attributes.severity_text') or
        extract_field_value(entry, 'severity_text') or
        extract_field_value(entry, 'severity')
    )
    body = (
        extract_field_value(entry, 'attributes.body') or
        extract_field_value(entry, 'body') or
        extract_field_value(entry, 'message')
    )
    
    # æå–ä¸¥é‡ç¨‹åº¦æ•°å­—
    severity_number_str = (
        extract_field_value(entry, 'attributes.severity_number') or
        extract_field_value(entry, 'severity_number')
    )
    severity_number = int(severity_number_str) if severity_number_str and severity_number_str.isdigit() else None
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºé”™è¯¯ï¼ˆä½¿ç”¨signoz_schemaæ¨¡å—ï¼‰
    try:
        from signoz_schema import is_error_severity
        is_error = is_error_severity(severity, severity_number)
    except ImportError:
        # å¦‚æœæ¨¡å—ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç®€å•åˆ¤æ–­
        is_error = (
            (severity and ('error' in severity.lower() or 'å¼‚å¸¸' in severity or 'é”™è¯¯' in severity)) or
            (severity_number and severity_number >= 17)  # ERRORçº§åˆ«åŠä»¥ä¸Š
        )
    
    if is_error:
        log_analysis['error_count'] += 1
        
        # ç»Ÿè®¡é”™è¯¯ç±»å‹
        error_type = extract_error_type(body or severity)
        if error_type:
            log_analysis['error_types'][error_type] = log_analysis['error_types'].get(error_type, 0) + 1
        
        # æå–å…³é”®é”™è¯¯
        if body:
            key_error = {
                'service': service_name,
                'error': body[:200],  # é™åˆ¶é•¿åº¦
                'severity': severity
            }
            log_analysis['key_errors'].append(key_error)


def extract_field_value(entry: Dict[str, Any], field_name: str) -> Optional[str]:
    """
    æå–å­—æ®µå€¼ï¼ˆæ”¯æŒå¤šç§æ•°æ®ç»“æ„ï¼‰
    
    æ ¹æ®SigNozæ•°æ®ç»“æ„ï¼Œå­—æ®µå¯èƒ½ä½äºï¼š
    - resources.service.nameï¼ˆæ³¨æ„æ˜¯å¤æ•°resourcesï¼‰
    - attributes.body
    - attributes.user.idï¼ˆåµŒå¥—å­—æ®µï¼‰
    - attributes.user.client_idï¼ˆåµŒå¥—å­—æ®µï¼‰
    ç­‰ä½ç½®
    """
    # ç›´æ¥å­—æ®µ
    if field_name in entry:
        value = entry[field_name]
        if isinstance(value, (str, int, float)):
            return str(value)
    
    # æ ¹æ®å­—æ®µè·¯å¾„æå–
    if '.' in field_name:
        parts = field_name.split('.')
        
        # å¦‚æœæ˜¯ resources.xxx æˆ– attributes.xxx æ ¼å¼ï¼ˆæ³¨æ„resourcesæ˜¯å¤æ•°ï¼‰
        if parts[0] in ['resource', 'resources', 'attributes', 'span', 'log']:
            context = parts[0]
            field_key = '.'.join(parts[1:])
            
            # å¤„ç†resourcesï¼ˆå¤æ•°ï¼‰å’Œresourceï¼ˆå•æ•°ï¼‰çš„å…¼å®¹
            context_key = 'resources' if context == 'resource' else context
            if context_key in entry and isinstance(entry[context_key], dict):
                current = entry[context_key]
                # å¤„ç†åµŒå¥—å­—æ®µï¼ˆå¦‚ service.name, user.idï¼‰
                if '.' in field_key:
                    for part in field_key.split('.'):
                        if isinstance(current, dict) and part in current:
                            current = current[part]
                        else:
                            return None
                    if isinstance(current, (str, int, float)):
                        return str(current)
                else:
                    if field_key in current:
                        value = current[field_key]
                        if isinstance(value, (str, int, float)):
                            return str(value)
        else:
            # æ™®é€šåµŒå¥—å­—æ®µï¼ˆå¦‚ user.id, user.client_idï¼‰
            # ä¼˜å…ˆä»attributesä¸­æŸ¥æ‰¾
            if 'attributes' in entry and isinstance(entry['attributes'], dict):
                current = entry['attributes']
                for part in parts:
                    if isinstance(current, dict) and part in current:
                        current = current[part]
                    else:
                        break
                else:
                    if isinstance(current, (str, int, float)):
                        return str(current)
            
            # å¦‚æœattributesä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ ¹å¯¹è±¡æŸ¥æ‰¾
            current = entry
            for part in parts:
                if isinstance(current, dict) and part in current:
                    current = current[part]
                else:
                    return None
            if isinstance(current, (str, int, float)):
                return str(current)
    
    # å°è¯•ä»resourcesä¸­æå–ï¼ˆæ³¨æ„æ˜¯å¤æ•°ï¼‰
    if 'resources' in entry and isinstance(entry['resources'], dict):
        if field_name in entry['resources']:
            value = entry['resources'][field_name]
            if isinstance(value, (str, int, float)):
                return str(value)
        # å°è¯•åµŒå¥—å­—æ®µï¼ˆå¦‚ service.nameï¼‰
        if '.' in field_name:
            parts = field_name.split('.')
            if parts[0] in entry['resources']:
                current = entry['resources'][parts[0]]
                for part in parts[1:]:
                    if isinstance(current, dict) and part in current:
                        current = current[part]
                    else:
                        break
                else:
                    if isinstance(current, (str, int, float)):
                        return str(current)
    
    # å…¼å®¹resourceï¼ˆå•æ•°ï¼‰æ ¼å¼
    if 'resource' in entry and isinstance(entry['resource'], dict):
        if field_name in entry['resource']:
            value = entry['resource'][field_name]
            if isinstance(value, (str, int, float)):
                return str(value)
        # å°è¯•åµŒå¥—å­—æ®µï¼ˆå¦‚ service.nameï¼‰
        if '.' in field_name:
            parts = field_name.split('.')
            if parts[0] in entry['resource']:
                current = entry['resource'][parts[0]]
                for part in parts[1:]:
                    if isinstance(current, dict) and part in current:
                        current = current[part]
                    else:
                        break
                else:
                    if isinstance(current, (str, int, float)):
                        return str(current)
    
    # å°è¯•ä»attributesä¸­æå–
    if 'attributes' in entry and isinstance(entry['attributes'], dict):
        if field_name in entry['attributes']:
            value = entry['attributes'][field_name]
            if isinstance(value, (str, int, float)):
                return str(value)
        # å°è¯•åµŒå¥—å­—æ®µï¼ˆå¦‚ user.id, user.client_idï¼‰
        if '.' in field_name:
            parts = field_name.split('.')
            if parts[0] in entry['attributes']:
                current = entry['attributes'][parts[0]]
                for part in parts[1:]:
                    if isinstance(current, dict) and part in current:
                        current = current[part]
                    else:
                        break
                else:
                    if isinstance(current, (str, int, float)):
                        return str(current)
    
    return None


def extract_error_type(error_text: str) -> Optional[str]:
    """æå–é”™è¯¯ç±»å‹"""
    if not error_text:
        return None
    
    error_text_lower = error_text.lower()
    
    # å¸¸è§é”™è¯¯ç±»å‹
    error_patterns = {
        'timeout': ['timeout', 'è¶…æ—¶'],
        'connection': ['connection', 'è¿æ¥', 'connect'],
        'permission': ['permission', 'æƒé™', 'forbidden'],
        'not found': ['not found', '404', 'æœªæ‰¾åˆ°'],
        'server error': ['500', 'server error', 'æœåŠ¡å™¨é”™è¯¯'],
        'validation': ['validation', 'éªŒè¯', 'invalid'],
        'database': ['database', 'æ•°æ®åº“', 'sql']
    }
    
    for error_type, patterns in error_patterns.items():
        for pattern in patterns:
            if pattern in error_text_lower:
                return error_type
    
    return 'unknown'


def generate_log_summary(log_analysis: Dict[str, Any]) -> str:
    """ç”Ÿæˆæ—¥å¿—åˆ†ææ‘˜è¦"""
    summary_parts = []
    
    if log_analysis['error_count'] > 0:
        summary_parts.append(f"å‘ç° {log_analysis['error_count']} ä¸ªé”™è¯¯")
    
    if log_analysis['error_types']:
        error_types_str = ', '.join([f"{k}({v})" for k, v in log_analysis['error_types'].items()])
        summary_parts.append(f"é”™è¯¯ç±»å‹: {error_types_str}")
    
    if log_analysis['services']:
        services_str = ', '.join(log_analysis['services'])
        summary_parts.append(f"æ¶‰åŠæœåŠ¡: {services_str}")
    
    if log_analysis['key_errors']:
        summary_parts.append(f"å…³é”®é”™è¯¯: {len(log_analysis['key_errors'])} æ¡")
    
    return "; ".join(summary_parts) if summary_parts else "æœªå‘ç°æ˜æ˜¾é”™è¯¯"


def analyze_code_logic(
    log_analysis: Dict[str, Any],
    project_path: str,
    ticket_info: Dict[str, Any]
) -> Dict[str, Any]:
    """
    åˆ†æä»£ç é€»è¾‘ï¼ŒåŸºäºé”™è¯¯ä¿¡æ¯å®šä½ä»£ç æ–‡ä»¶
    
    Args:
        log_analysis: æ—¥å¿—åˆ†æç»“æœ
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        ticket_info: å·¥å•ä¿¡æ¯
    
    Returns:
        ä»£ç åˆ†æç»“æœ
    """
    code_analysis = {
        'related_files': [],
        'error_locations': [],
        'suggestions': []
    }
    
    project_root = Path(project_path).resolve()
    
    # æ ¹æ®é”™è¯¯ä¿¡æ¯å®šä½ä»£ç æ–‡ä»¶
    key_errors = log_analysis.get('key_errors', [])
    for error in key_errors[:10]:  # é™åˆ¶æ•°é‡
        error_text = error.get('error', '')
        service = error.get('service', '')
        
        # æŸ¥æ‰¾ç›¸å…³ä»£ç æ–‡ä»¶
        related_files = find_related_code_files(project_root, error_text, service)
        code_analysis['related_files'].extend(related_files)
        
        # è®°å½•é”™è¯¯ä½ç½®
        if related_files:
            code_analysis['error_locations'].append({
                'error': error_text[:100],
                'files': related_files
            })
    
    # å»é‡
    code_analysis['related_files'] = list(set(code_analysis['related_files']))
    
    return code_analysis


def find_related_code_files(project_root: Path, error_text: str, service: str) -> list:
    """æŸ¥æ‰¾ç›¸å…³ä»£ç æ–‡ä»¶"""
    related_files = []
    
    # æå–é”™è¯¯å…³é”®è¯
    keywords = extract_code_keywords(error_text)
    
    # æ ¹æ®æœåŠ¡åæŸ¥æ‰¾æ–‡ä»¶
    if service:
        service_patterns = [
            f'**/{service}/**/*.py',
            f'**/{service}/**/*.js',
            f'**/{service}/**/*.ts',
            f'**/*{service}*.py',
            f'**/*{service}*.js',
            f'**/*{service}*.ts'
        ]
        
        for pattern in service_patterns:
            for file_path in project_root.rglob(pattern):
                if file_path.is_file():
                    rel_path = str(file_path.relative_to(project_root))
                    if rel_path not in related_files:
                        related_files.append(rel_path)
                    if len(related_files) >= 10:  # é™åˆ¶æ•°é‡
                        return related_files
    
    # æ ¹æ®å…³é”®è¯æŸ¥æ‰¾æ–‡ä»¶
    for keyword in keywords[:3]:  # é™åˆ¶å…³é”®è¯æ•°é‡
        if len(keyword) < 3:  # è·³è¿‡å¤ªçŸ­çš„å…³é”®è¯
            continue
        
        patterns = [
            f'**/*{keyword}*.py',
            f'**/*{keyword}*.js',
            f'**/*{keyword}*.ts'
        ]
        
        for pattern in patterns:
            for file_path in project_root.rglob(pattern):
                if file_path.is_file():
                    rel_path = str(file_path.relative_to(project_root))
                    if rel_path not in related_files:
                        related_files.append(rel_path)
                    if len(related_files) >= 10:
                        return related_files
    
    return related_files[:10]  # é™åˆ¶æ€»æ•°


def extract_code_keywords(error_text: str) -> list:
    """æå–ä»£ç å…³é”®è¯"""
    keywords = []
    
    # æå–å‡½æ•°åã€ç±»åç­‰
    patterns = [
        r'(\w+Error)',
        r'(\w+Exception)',
        r'(\w+Failed)',
        r'function\s+(\w+)',
        r'class\s+(\w+)',
        r'def\s+(\w+)',
        r'class\s+(\w+)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, error_text, re.IGNORECASE)
        keywords.extend(matches)
    
    return list(set(keywords))


def generate_solution(
    log_analysis: Dict[str, Any],
    code_analysis: Dict[str, Any],
    ticket_info: Dict[str, Any],
    history_experiences: list,
    prevalence_analysis: Optional[Dict[str, Any]] = None
) -> str:
    """
    ç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ
    
    Args:
        log_analysis: æ—¥å¿—åˆ†æç»“æœ
        code_analysis: ä»£ç åˆ†æç»“æœ
        ticket_info: å·¥å•ä¿¡æ¯
        history_experiences: å†å²ç»éªŒåˆ—è¡¨
    
    Returns:
        è§£å†³æ–¹æ¡ˆæ–‡æœ¬
    """
    solution_parts = []
    
    solution_parts.append("# é—®é¢˜åˆ†æ")
    solution_parts.append("")
    solution_parts.append(f"## æ—¥å¿—åˆ†ææ‘˜è¦")
    solution_parts.append(f"{log_analysis.get('summary', 'æ— ')}")
    solution_parts.append("")
    
    if log_analysis.get('error_types'):
        solution_parts.append("## é”™è¯¯ç±»å‹ç»Ÿè®¡")
        for error_type, count in log_analysis['error_types'].items():
            solution_parts.append(f"- {error_type}: {count} æ¬¡")
        solution_parts.append("")
    
    if code_analysis.get('related_files'):
        solution_parts.append("## ç›¸å…³ä»£ç æ–‡ä»¶")
        for file_path in code_analysis['related_files'][:10]:
            solution_parts.append(f"- {file_path}")
        solution_parts.append("")
    
    if history_experiences:
        solution_parts.append("## å‚è€ƒå†å²ç»éªŒ")
        for i, exp in enumerate(history_experiences[:3], 1):
            solution_parts.append(f"### ç»éªŒ {i} (ç›¸ä¼¼åº¦: {exp.get('similarity', 0):.2%})")
            solution_parts.append(f"**é—®é¢˜**: {exp.get('problem_description', '')[:200]}...")
            solution_parts.append(f"**è§£å†³æ–¹æ¡ˆ**: {exp.get('solution', '')[:200]}...")
            solution_parts.append("")
    
    # æ·»åŠ æ™®éæ€§é—®é¢˜åˆ†æ
    if prevalence_analysis:
        solution_parts.append("## âš ï¸ æ™®éæ€§é—®é¢˜åˆ†æ")
        solution_parts.append("")
        if prevalence_analysis.get('is_prevalent'):
            prevalence_level = prevalence_analysis.get('prevalence_level', 'unknown')
            level_labels = {
                'critical': 'ğŸ”´ ä¸¥é‡',
                'high': 'ğŸŸ  é«˜',
                'medium': 'ğŸŸ¡ ä¸­ç­‰',
                'low': 'ğŸŸ¢ è½»å¾®'
            }
            level_label = level_labels.get(prevalence_level, 'æœªçŸ¥')
            solution_parts.append(f"### {level_label} æ™®éæ€§é—®é¢˜")
            solution_parts.append("")
            solution_parts.append("**æ­¤é—®é¢˜å¯èƒ½æ˜¯æ™®éæ€§é—®é¢˜ï¼Œå½±å“èŒƒå›´è¾ƒå¹¿ï¼š**")
            solution_parts.append("")
            key_indicators = prevalence_analysis.get('key_indicators', [])
            for indicator in key_indicators:
                solution_parts.append(f"- {indicator}")
            solution_parts.append("")
            if prevalence_analysis.get('affected_countries'):
                countries = prevalence_analysis.get('affected_countries', [])
                solution_parts.append(f"**å—å½±å“å›½å®¶/åœ°åŒº**: {', '.join(countries[:10])}")
                solution_parts.append("")
            if prevalence_analysis.get('affected_cities'):
                cities = prevalence_analysis.get('affected_cities', [])
                solution_parts.append(f"**å—å½±å“åŸå¸‚**: {', '.join(cities[:10])}")
                solution_parts.append("")
            recommendation = prevalence_analysis.get('recommendation', '')
            if recommendation:
                solution_parts.append(recommendation)
                solution_parts.append("")
        else:
            solution_parts.append("### âœ… å­¤ç«‹äº‹ä»¶")
            solution_parts.append("")
            solution_parts.append("æ­¤é—®é¢˜ä¼¼ä¹æ˜¯å­¤ç«‹äº‹ä»¶ï¼Œå½±å“èŒƒå›´æœ‰é™ã€‚")
            solution_parts.append("")
    
    solution_parts.append("# è§£å†³æ–¹æ¡ˆå»ºè®®")
    solution_parts.append("")
    solution_parts.append("åŸºäºä»¥ä¸Šåˆ†æï¼Œå»ºè®®é‡‡å–ä»¥ä¸‹æªæ–½ï¼š")
    solution_parts.append("")
    solution_parts.append("1. æ£€æŸ¥ç›¸å…³ä»£ç æ–‡ä»¶ï¼Œç¡®è®¤é”™è¯¯åŸå› ")
    solution_parts.append("2. å‚è€ƒå†å²ç»éªŒï¼Œé‡‡ç”¨å·²éªŒè¯çš„è§£å†³æ–¹æ¡ˆ")
    solution_parts.append("3. å¦‚æœé—®é¢˜æŒç»­ï¼Œè€ƒè™‘æ‰©å¤§æŸ¥è¯¢æ—¶é—´èŒƒå›´æˆ–æ£€æŸ¥å…¶ä»–æœåŠ¡")
    solution_parts.append("")
    
    return "\n".join(solution_parts)


def generate_solution_document(
    solution: str,
    ticket_context: Dict[str, Any],
    project_path: str,
    ticket_id: str,
    prevalence_analysis: Optional[Dict[str, Any]] = None
) -> Optional[Path]:
    """
    ç”Ÿæˆè§£å†³æ–¹æ¡ˆæ–‡æ¡£
    
    Args:
        solution: è§£å†³æ–¹æ¡ˆæ–‡æœ¬
        ticket_context: å·¥å•ä¸Šä¸‹æ–‡
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        ticket_id: å·¥å•ID
    
    Returns:
        ä¿å­˜çš„æ–‡æ¡£è·¯å¾„ï¼Œå¦‚æœä¿å­˜å¤±è´¥åˆ™è¿”å›None
    """
    ticket_dir = get_ticket_dir(project_path, ticket_id)
    solution_file = ticket_dir / 'solution.md'
    
    # æ„å»ºå®Œæ•´æ–‡æ¡£
    ticket_info = ticket_context.get('ticket_info', {})
    time_range = ticket_context.get('time_range', {})
    
    # æ„å»ºæ–‡æ¡£å¤´éƒ¨ï¼Œå¦‚æœæ˜¯æ™®éæ€§é—®é¢˜åˆ™ç‰¹åˆ«æ ‡æ³¨
    header = "# å·¥å•åˆ†æè§£å†³æ–¹æ¡ˆ"
    if prevalence_analysis and prevalence_analysis.get('is_prevalent'):
        prevalence_level = prevalence_analysis.get('prevalence_level', 'unknown')
        level_labels = {
            'critical': 'ğŸ”´ ä¸¥é‡æ™®éæ€§é—®é¢˜',
            'high': 'ğŸŸ  é«˜æ™®éæ€§é—®é¢˜',
            'medium': 'ğŸŸ¡ ä¸­ç­‰æ™®éæ€§é—®é¢˜',
            'low': 'ğŸŸ¢ è½»å¾®æ™®éæ€§é—®é¢˜'
        }
        level_label = level_labels.get(prevalence_level, 'æ™®éæ€§é—®é¢˜')
        header = f"# å·¥å•åˆ†æè§£å†³æ–¹æ¡ˆ - âš ï¸ {level_label}"
    
    document = f"""{header}

## å·¥å•ä¿¡æ¯

- **å·¥å•ID**: {ticket_id}
- **é—®é¢˜æè¿°**: {ticket_info.get('description', '')[:200]}...
- **æŸ¥è¯¢æ—¶é—´èŒƒå›´**: {time_range.get('start_display', '')} - {time_range.get('end_display', '')}
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    # å¦‚æœæ˜¯æ™®éæ€§é—®é¢˜ï¼Œåœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ è­¦å‘Šæ¡†
    if prevalence_analysis and prevalence_analysis.get('is_prevalent'):
        document += f"""
---

## âš ï¸ æ™®éæ€§é—®é¢˜è­¦å‘Š

**æ­¤é—®é¢˜å·²è¢«è¯†åˆ«ä¸ºæ™®éæ€§é—®é¢˜ï¼Œå½±å“èŒƒå›´è¾ƒå¹¿ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†ï¼**

- **æ™®éæ€§çº§åˆ«**: {prevalence_analysis.get('prevalence_level', 'unknown')}
- **å½±å“é”™è¯¯æ•°**: {prevalence_analysis.get('affected_count', 0)}
- **å½±å“ç”¨æˆ·æ•°**: {len(prevalence_analysis.get('affected_users', []))}
- **å½±å“è®¾å¤‡æ•°**: {len(prevalence_analysis.get('affected_devices', []))}
"""
        if prevalence_analysis.get('affected_countries'):
            countries = prevalence_analysis.get('affected_countries', [])
            document += f"- **å—å½±å“å›½å®¶/åœ°åŒº**: {', '.join(countries[:10])}\n"
        if prevalence_analysis.get('affected_cities'):
            cities = prevalence_analysis.get('affected_cities', [])
            document += f"- **å—å½±å“åŸå¸‚**: {', '.join(cities[:10])}\n"
        document += f"\n**å»ºè®®**: {prevalence_analysis.get('recommendation', '')}\n"
    
    document += f"""
---

{solution}

---

## åç»­æ­¥éª¤

1. æ ¹æ®è§£å†³æ–¹æ¡ˆå»ºè®®è¿›è¡Œé—®é¢˜ä¿®å¤
2. éªŒè¯ä¿®å¤æ•ˆæœ
3. å¦‚æœé—®é¢˜å·²è§£å†³ï¼Œå¯ä»¥å°†æ­¤ç»éªŒä¿å­˜åˆ°ç»éªŒåº“
"""
    
    if save_markdown_file(solution_file, document):
        return solution_file
    return None


def init_phase_2(
    project_path: str,
    ticket_id: str,
    ticket_context: Dict[str, Any]
) -> Dict[str, Any]:
    """
    é˜¶æ®µ2ä¸»å‡½æ•°ï¼šç»¼åˆåˆ†æ
    
    Args:
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        ticket_id: å·¥å•ID
        ticket_context: å·¥å•ä¸Šä¸‹æ–‡
    
    Returns:
        åˆ†æç»“æœ
    """
    print("\n" + "="*60)
    print("ğŸ“‹ é˜¶æ®µ2ï¼šç»¼åˆåˆ†æ")
    print("="*60)
    
    from mcp_handler import load_mcp_results
    
    # åŠ è½½MCPæŸ¥è¯¢ç»“æœ
    print("\nğŸ“‚ åŠ è½½MCPæŸ¥è¯¢ç»“æœ...")
    mcp_results = load_mcp_results(project_path, ticket_id)
    if not mcp_results:
        print("  âš ï¸  MCPæŸ¥è¯¢ç»“æœä¸å­˜åœ¨ï¼Œè¯·å…ˆæ‰§è¡ŒMCPæŸ¥è¯¢")
        return {}
    
    print("  âœ… å·²åŠ è½½MCPæŸ¥è¯¢ç»“æœ")
    
    # æ£€æŸ¥æŸ¥è¯¢ç»“æœæ˜¯å¦ä¸ºç©º
    is_empty, warnings = check_mcp_results_empty(mcp_results)
    
    if is_empty:
        print("\nâš ï¸  æŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œç”Ÿæˆåˆæ­¥åˆ¤æ–­...")
        print("  âš ï¸  æœªæ‰¾åˆ°ç›¸å…³æ—¥å¿—æ•°æ®")
        if warnings:
            print("  âš ï¸  æŸ¥è¯¢è­¦å‘Šï¼š")
            for warning in warnings:
                print(f"     - {warning}")
        
        # ç”Ÿæˆåˆæ­¥åˆ¤æ–­
        preliminary_analysis = generate_preliminary_analysis(
            ticket_info,
            ticket_context,
            mcp_results,
            warnings
        )
        
        # ç”Ÿæˆåˆæ­¥åˆ¤æ–­æ–‡æ¡£
        print("\nğŸ“ ç”Ÿæˆåˆæ­¥åˆ¤æ–­æ–‡æ¡£...")
        ticket_dir = get_ticket_dir(project_path, ticket_id)
        preliminary_file = ticket_dir / 'preliminary_analysis.md'
        
        document = f"""# å·¥å•åˆæ­¥åˆ¤æ–­

## å·¥å•ä¿¡æ¯

- **å·¥å•ID**: {ticket_id}
- **é—®é¢˜æè¿°**: {ticket_info.get('description', '')[:200]}...
- **æŸ¥è¯¢æ—¶é—´èŒƒå›´**: {ticket_context.get('time_range', {}).get('start_display', '')} - {ticket_context.get('time_range', {}).get('end_display', '')}
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{preliminary_analysis}

---

## âš ï¸ é‡è¦æç¤º

**ç”±äºæŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´è§£å†³æ–¹æ¡ˆã€‚**

è¯·æ ¹æ®ä¸Šè¿°å»ºè®®æä¾›æ›´ç²¾ç¡®çš„ä¿¡æ¯ï¼Œç„¶åï¼š
1. æ›´æ–°å·¥å•ä¿¡æ¯ï¼ˆæ·»åŠ ç”¨æˆ·IDã€è®¾å¤‡IDã€æ¥å£è·¯å¾„ç­‰ï¼‰
2. è°ƒæ•´æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼ˆå¦‚æœéœ€è¦ï¼‰
3. é‡æ–°è¿è¡Œåˆ†æ

æˆ–è€…ï¼Œå¦‚æœæ‚¨ç¡®è®¤ä¿¡æ¯æ— è¯¯ï¼Œå¯ä»¥ï¼š
- æ£€æŸ¥SigNozå¹³å°ï¼Œç¡®è®¤è¯¥æ—¶é—´æ®µå†…æ˜¯å¦æœ‰ç›¸å…³æ—¥å¿—
- ç¡®è®¤æœåŠ¡åç§°ã€æ—¶é—´èŒƒå›´ç­‰ä¿¡æ¯æ˜¯å¦æ­£ç¡®
- å¦‚æœç¡®å®æ²¡æœ‰æ—¥å¿—ï¼Œå¯èƒ½éœ€è¦æ‰©å¤§æ—¶é—´èŒƒå›´æˆ–æ£€æŸ¥å…¶ä»–æœåŠ¡
"""
        
        if save_markdown_file(preliminary_file, document):
            print(f"  âœ… åˆæ­¥åˆ¤æ–­æ–‡æ¡£å·²ä¿å­˜: {preliminary_file}")
        else:
            print("  âš ï¸  åˆæ­¥åˆ¤æ–­æ–‡æ¡£ä¿å­˜å¤±è´¥")
        
        print("\n" + "="*60)
        print("âš ï¸  æŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œå·²ç”Ÿæˆåˆæ­¥åˆ¤æ–­")
        print("="*60)
        print("\nè¯·æŸ¥çœ‹åˆæ­¥åˆ¤æ–­æ–‡æ¡£ï¼Œæä¾›æ›´ç²¾ç¡®çš„ä¿¡æ¯åé‡æ–°è¿è¡Œåˆ†æã€‚")
        print(f"æ–‡æ¡£ä½ç½®: {preliminary_file}")
        
        return {
            'log_analysis': {'has_data': False, 'summary': 'æŸ¥è¯¢ç»“æœä¸ºç©º'},
            'is_empty': True,
            'preliminary_analysis': preliminary_analysis,
            'preliminary_file': str(preliminary_file),
            'warnings': warnings
        }
    
    # å¤„ç†æ—¥å¿—æ•°æ®
    print("\nğŸ“Š å¤„ç†æ—¥å¿—æ•°æ®...")
    log_analysis = process_log_data(mcp_results)
    print(f"  âœ… æ—¥å¿—åˆ†æå®Œæˆ: {log_analysis.get('summary', '')}")
    
    # åˆ†æä»£ç é€»è¾‘
    print("\nğŸ’» åˆ†æä»£ç é€»è¾‘...")
    ticket_info = ticket_context.get('ticket_info', {})
    code_analysis = analyze_code_logic(log_analysis, project_path, ticket_info)
    print(f"  âœ… ä»£ç åˆ†æå®Œæˆ: å‘ç° {len(code_analysis.get('related_files', []))} ä¸ªç›¸å…³æ–‡ä»¶")
    
    # æ£€ç´¢å†å²ç»éªŒ
    print("\nğŸ§  æ£€ç´¢å†å²ç»éªŒ...")
    problem_description = ticket_info.get('description', '')
    history_experiences = search_history_experience(project_path, problem_description)
    print(f"  âœ… æ£€ç´¢åˆ° {len(history_experiences)} æ¡ç›¸ä¼¼ç»éªŒ")
    
    # åˆ†ææ™®éæ€§é—®é¢˜
    print("\nğŸ” åˆ†ææ™®éæ€§é—®é¢˜...")
    prevalence_result = analyze_prevalence(
        ticket_info,
        log_analysis,
        ticket_context,
        project_path,
        ticket_id
    )
    
    # å¦‚æœæ™®éæ€§æŸ¥è¯¢æŒ‡ä»¤å·²ç”Ÿæˆï¼Œå°è¯•åŠ è½½ç»“æœ
    prevalence_analysis = None
    if prevalence_result.get('status') == 'pending_ai_execution':
        # æ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢ç»“æœ
        from pathlib import Path
        ticket_dir = get_ticket_dir(project_path, ticket_id)
        prevalence_results_file = ticket_dir / 'prevalence_results.json'
        if prevalence_results_file.exists():
            print("  ğŸ“Š å‘ç°æ™®éæ€§æŸ¥è¯¢ç»“æœï¼Œè¿›è¡Œåˆ†æ...")
            prevalence_analysis = load_and_analyze_prevalence_results(
                project_path,
                ticket_id,
                ticket_info,
                prevalence_result.get('features', {})
            )
            if prevalence_analysis.get('is_prevalent'):
                print(f"  âš ï¸  æ£€æµ‹åˆ°æ™®éæ€§é—®é¢˜ï¼çº§åˆ«: {prevalence_analysis.get('prevalence_level', 'unknown')}")
                print(f"     å½±å“: {prevalence_analysis.get('affected_count', 0)} ä¸ªé”™è¯¯, {len(prevalence_analysis.get('affected_users', []))} ä¸ªç”¨æˆ·")
            else:
                print("  âœ… æœªæ£€æµ‹åˆ°æ™®éæ€§é—®é¢˜ï¼Œä¼¼ä¹æ˜¯å­¤ç«‹äº‹ä»¶")
        else:
            print("  â³ ç­‰å¾…AIæ‰§è¡Œæ™®éæ€§æŸ¥è¯¢ï¼ˆprevalence_instructions.jsonï¼‰")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if not log_analysis.get('has_data', True):
        print("\nâš ï¸  æ—¥å¿—åˆ†ææœªå‘ç°æ•°æ®ï¼Œä½†æŸ¥è¯¢ç»“æœä¸ä¸ºç©º")
        print("  å¯èƒ½æ˜¯æŸ¥è¯¢æ¡ä»¶è¿‡äºä¸¥æ ¼ï¼Œå»ºè®®æ”¾å®½æŸ¥è¯¢æ¡ä»¶")
    
    # ç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ
    print("\nğŸ’¡ ç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ...")
    solution = generate_solution(
        log_analysis,
        code_analysis,
        ticket_info,
        history_experiences,
        prevalence_analysis
    )
    
    # ç”Ÿæˆè§£å†³æ–¹æ¡ˆæ–‡æ¡£
    print("\nğŸ“ ç”Ÿæˆè§£å†³æ–¹æ¡ˆæ–‡æ¡£...")
    solution_file = generate_solution_document(
        solution,
        ticket_context,
        project_path,
        ticket_id,
        prevalence_analysis
    )
    if solution_file:
        print(f"  âœ… è§£å†³æ–¹æ¡ˆæ–‡æ¡£å·²ä¿å­˜: {solution_file}")
    else:
        print("  âš ï¸  è§£å†³æ–¹æ¡ˆæ–‡æ¡£ä¿å­˜å¤±è´¥")
    
    # ä¿å­˜ç»éªŒï¼ˆå¯é€‰ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤ï¼‰
    print("\nğŸ’¾ ä¿å­˜ç»éªŒ...")
    services = ticket_info.get('services', [])
    tags = ticket_info.get('keywords', [])
    experience_file = save_experience(
        project_path,
        problem_description,
        solution,
        success=True,  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè®¾ç½®
        services=services,
        tags=tags
    )
    if experience_file:
        print(f"  âœ… ç»éªŒå·²ä¿å­˜: {experience_file}")
    else:
        print("  âš ï¸  ç»éªŒä¿å­˜å¤±è´¥")
    
    print("\n" + "="*60)
    
    return {
        'log_analysis': log_analysis,
        'code_analysis': code_analysis,
        'history_experiences': history_experiences,
        'prevalence_analysis': prevalence_analysis,
        'solution': solution,
        'solution_file': str(solution_file) if solution_file else None,
        'experience_file': str(experience_file) if experience_file else None
    }
